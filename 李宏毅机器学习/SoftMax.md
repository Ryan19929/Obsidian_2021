![[Pasted image 20211006145051.png]]
- 目前感觉像是归一化
- ![[Pasted image 20211030185326.png]]
- Mean Square Errot(MSE)
- [[Cross Entropy(交叉熵)|交叉熵在pytorch和softmax绑定]] 
- ![[Pasted image 20211030192057.png]]
	- 使用 MSE 在一开始loss很平坦但是loss 很大的情况下就没有办法